{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market News Sentiment\n",
    "This project looks into classifying the sentiment of stock market news and tweets into a bullish or bearish outlook. The data is from [Kaggle: Stock-Market Sentiment Dataset](https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset).\n",
    "\n",
    "The data is broken up into two columns: `Text` that is a block of text and `Sentiment` that is a value between -1 and 1 value, meaning -1 is negative sentiment, 0 is neutral, and 1 is positive sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Import the necessary dependencies and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import pyprind\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import RNN, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Num_Words'] = data['Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "max_words = data['Num_Words'].max()\n",
    "min_words = data['Num_Words'].min()\n",
    "avg_words = data['Num_Words'].mean()\n",
    "\n",
    "print(f\"Max words: {max_words}\\nMin words: {min_words},\\nAvg words: {avg_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = data['Sentiment'].value_counts()\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Sentiment', data=data)\n",
    "plt.title('Distribution of Labels in Training Set')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "avg_words_per_category = data.groupby('Sentiment')['Num_Words'].mean()\n",
    "\n",
    "avg_words_per_category.plot(kind='bar')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Average Number of Words')\n",
    "plt.title('Average Words per Sentiment')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We need to preprocess the data to make it readable for the model. \n",
    "\n",
    "**Steps**:\n",
    "1. We need to drop all duplicates \n",
    "\n",
    "2. We need to remove all the stop words in the text using `nltk`'s stopwords.\n",
    "\n",
    "3. We need to remove HTML tags, URLs, special characters, and convert to lowercase. \n",
    "\n",
    "4. We need to find unique words in dataset (using `Counter()`) to count each word's occurence and convert text to integers. Then we will create a mapping to map each unique word to integer. \n",
    "\n",
    "5. We need to make the sequences same length. The sequences that don't meet that length will be padded with '0's' while the longer ones will be cut. We will define same-length sequences by:\n",
    "    - If sequence length < 200: left-pad with zeros\n",
    "    - If sequence length > 200: use the last 200 elements\n",
    "\n",
    "6. Use `train_test_split` to get an 80/20 split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['Text'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "data['Text'] = data['Text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(data['Text']),\n",
    "                       title='Counting words occurences')\n",
    "for i, sentiment in enumerate(data['Text']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' for c in sentiment]).lower()\n",
    "    data.loc[i,'Text'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "\n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_sentiment = []\n",
    "pbar = pyprind.ProgBar(len(data['Text']),\n",
    "                       title='Map sentiments to ints')\n",
    "for sentiment in data['Text']:\n",
    "    mapped_sentiment.append([word_to_int[word] for word in sentiment.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200  \n",
    "sequences = np.zeros((len(mapped_sentiment), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_sentiment):\n",
    "    sentiment_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = sentiment_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences\n",
    "y = data['Sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function that breaks given datset into chunks and returns a generator to iterate through mini-batches\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "# Define a fxn to generate mini-batches\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The `SentimentRNN` class implements a recurrent neural network (RNN) for sentiment analysis using TensorFlow and Keras. It utilizes an **embedding layer** followed by **LSTM** (Long Short-Term Memory) layers with bidirectional processing to capture sequential dependencies effectively. Dropout regularization is applied to mitigate overfitting, and the final output layer uses a sigmoid activation function for binary classification. This architecture leverages the strengths of LSTM networks for handling sequential data and is suitable for tasks like sentiment analysis where understanding the context and dependencies among words is crucial for accurate classification:\n",
    "\n",
    "**Embedding Layer:**\n",
    "  - **Input:** \n",
    "    - Dimension of the vocabulary (`n_words`)\n",
    "    - Input sequence length (`seq_len`)\n",
    "  - **Functionality:** \n",
    "    - Maps each word index to a dense vector representation (`embed_size`)\n",
    "   \n",
    "**LSTM Layers:**\n",
    "  - **Bidirectional LSTM:**\n",
    "    - **Units:** `lstm_size`\n",
    "    - **Number of Layers:** `num_layers`\n",
    "    - **Return Sequences:** True for all layers except the last one\n",
    "    - **Functionality:** \n",
    "      - Captures forward and backward context of the input sequences\n",
    "      - Helps in understanding context from both past and future states\n",
    "\n",
    "**Dropout Layers:**\n",
    "  - **Usage:** \n",
    "    - Applied after each LSTM layer (`Dropout(0.5)`)\n",
    "    - Reduces overfitting by randomly setting a fraction of inputs to zero during training\n",
    "   \n",
    "**Dense Output Layer:**\n",
    "  - **Activation:** Sigmoid\n",
    "  - **Functionality:** \n",
    "    - Outputs a single probability score for binary sentiment classification\n",
    "\n",
    "**Optimizer and Loss Function:**\n",
    "  - **Optimizer:** Adam with a learning rate (`learning_rate`)\n",
    "  - **Loss Function:** Binary Cross-Entropy\n",
    "  - **Metrics:** Accuracy for evaluation during training\n",
    "   \n",
    "**Training:**\n",
    "  - Input: `X_train` (numpy array of shape `(num_samples, seq_len)`)\n",
    "  - Labels: `y_train` (numpy array of shape `(num_samples,)`)\n",
    "  - Trains the model using `num_epochs` with a batch size of `batch_size`\n",
    "  - Validates on 10% of the training data (`validation_split=0.1`)\n",
    "\n",
    "**Prediction:**\n",
    "  - Input: `X_data` (numpy array of shape `(num_samples, seq_len)`)\n",
    "  - Returns either predicted labels (`numpy.ndarray` of integers) or probabilities (`numpy.ndarray` of floats) based on the `return_proba` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN:\n",
    "    \"\"\"\n",
    "    A Recurrent Neural Network model for sentiment analysis using TensorFlow and Keras.\n",
    "\n",
    "    Methods:\n",
    "        build_model():\n",
    "            Builds the RNN model using TensorFlow and Keras.\n",
    "        \n",
    "        train(X_train, y_train, num_epochs):\n",
    "            Trains the RNN model on the provided training data.\n",
    "\n",
    "        predict(X_data, return_proba=False):\n",
    "            Makes predictions on new data.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentRNN object with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            n_words (int): Number of words in the vocabulary.\n",
    "            seq_len (int, optional): Length of input sequences (default is 200).\n",
    "            lstm_size (int, optional): Size of LSTM units (default is 256).\n",
    "            num_layers (int, optional): Number of LSTM layers (default is 1).\n",
    "            batch_size (int, optional): Batch size for training (default is 64).\n",
    "            learning_rate (float, optional): Learning rate for the optimizer (default is 0.0001).\n",
    "            embed_size (int, optional): Size of word embeddings (default is 200).\n",
    "        \"\"\"\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the LSTM-based sentiment analysis model using tf.keras.Sequential.\n",
    "        Embedding layer, LSTM layers (with dropout), and Dense output layer are added.\n",
    "        Adam optimizer is used with binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=self.n_words, output_dim=self.embed_size, input_length=self.seq_len))\n",
    "        for _ in range(self.num_layers):\n",
    "            self.model.add(Bidirectional(LSTM(self.lstm_size, return_sequences=True)))\n",
    "            self.model.add(Dropout(0.5))\n",
    "        self.model.add(Bidirectional(LSTM(self.lstm_size)))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        # self.model.summary()\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        \"\"\"\n",
    "        Trains the sentiment analysis model on the provided training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (numpy.ndarray): Training input data of shape (num_samples, seq_len).\n",
    "            y_train (numpy.ndarray): Training labels of shape (num_samples,).\n",
    "            num_epochs (int): Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "        self.model.fit(X_train, y_train, epochs=num_epochs, batch_size=self.batch_size, validation_split=0.1)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        \"\"\"\n",
    "        Predicts sentiment labels or probabilities for input data.\n",
    "\n",
    "        Args:\n",
    "            X_data (numpy.ndarray): Input data of shape (num_samples, seq_len).\n",
    "            return_proba (bool, optional): If True, returns predicted probabilities; otherwise, returns labels (default is False).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels (if return_proba=False) or probabilities (if return_proba=True).\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(X_data, batch_size=self.batch_size)\n",
    "        if return_proba:\n",
    "            return predictions\n",
    "        else:\n",
    "            return (predictions > 0.5).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum index value in X_train and X_test\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "# Update the n_words parameter in the SentimentRNN constructor\n",
    "rnn = SentimentRNN(n_words=n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(preds == y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Let's see if we can increase the accuracy of this model. We will use GridSearch, but first, we need to create a wrapper for our model in order to use it. \n",
    "\n",
    "We will be working on the following hyperparameters:\n",
    "- `seq_len`: Affects how much contextual information the model can capture from each input text\n",
    "- `lstm_size`: Directly impacts the model's capacity to learn complex patterns and dependencies within the data\n",
    "- `num_layers`: Allows the model to learn hierarchical representations of text data, potentially improving its ability to understand nuanced sentiment expressions\n",
    "- `batch_size`: Affects the gradient update dynamics and training stability\n",
    "- `learning_rate`: Controls how much to update the model in response to estimated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNNWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1,\n",
    "                 batch_size=64, learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, num_epochs=5):\n",
    "        \"\"\"Fit method for GridSearchCV compatibility.\n",
    "\n",
    "        Args:\n",
    "            X: X training data\n",
    "            y: y training data\n",
    "            num_epochs (int, optional): _description_. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, estimator=self, dtype=None)\n",
    "        \n",
    "        self.model = SentimentRNN(\n",
    "            n_words=self.n_words,\n",
    "            seq_len=self.seq_len,\n",
    "            lstm_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate=self.learning_rate,\n",
    "            embed_size=self.embed_size\n",
    "        )\n",
    "        \n",
    "        self.model.train(X, y, num_epochs=num_epochs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict method for GridSearchCV compatibility\n",
    "\n",
    "        Args:\n",
    "            X: Assumes X is the test data\n",
    "\n",
    "        Returns:\n",
    "            predictions: Predictions made by the model\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for the estimator. This is for GridSearchCV compatibility.\n",
    "\n",
    "        Args:\n",
    "            deep (bool, optional): Controls the depth of the attributes that are included in the returned dictionary of parameters. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            params: Dictionary of parameters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'n_words': self.n_words,\n",
    "            'seq_len': self.seq_len,\n",
    "            'lstm_size': self.lstm_size,\n",
    "            'num_layers': self.num_layers,\n",
    "            'batch_size': self.batch_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'embed_size': self.embed_size\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of the estimator. This is for GridSearchCV compatibility.\n",
    "\n",
    "        Returns:\n",
    "            params: Sets the dictionary of parameters\n",
    "        \"\"\"\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('rnn', SentimentRNNWrapper(n_words=n_words)),\n",
    "])\n",
    "\n",
    "# Define parameters for grid search\n",
    "parameters = {\n",
    "    'rnn__seq_len': [100, 200, 300],\n",
    "    'rnn__lstm_size': [128, 256, 512],\n",
    "    'rnn__num_layers': [1, 2, 3],\n",
    "    'rnn__batch_size': [32, 64, 128],\n",
    "    'rnn__learning_rate': [0.001, 0.0001],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, cv=3, verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model\n",
    "Now it's time to test the best model combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test accuracy: {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project, we developed a Sentiment Analysis model using a Recurrent Neural Network (RNN) architecture implemented with TensorFlow and Keras. The goal was to classify the sentiment of text data into positive or negative categories. Here’s a summary of our approach and findings:\n",
    "\n",
    "1. **Model Architecture**: We implemented an RNN with LSTM layers, which are well-suited for sequential data like text due to their ability to capture dependencies over time. The model included embedding layers for text representation, multiple LSTM layers for learning hierarchical features, and dropout layers for regularization.\n",
    "\n",
    "2. **Hyperparameter Tuning**: We utilized GridSearchCV to systematically search through different combinations of hyperparameters such as sequence length (`seq_len`), LSTM size (`lstm_size`), number of layers (`num_layers`), batch size (`batch_size`), and learning rate (learning_rate). The goal was to optimize model performance by finding the best configuration for these parameters.\n",
    "\n",
    "3. **Training and Evaluation**: The model was trained on a dataset containing labeled text data for sentiment analysis. We evaluated the model’s performance using metrics like accuracy to measure its ability to correctly predict sentiment labels on unseen data.\n",
    "\n",
    "4. **Performance**: Through iterative tuning and evaluation, we observed improvements in the model’s accuracy as we optimized hyperparameters. This process allowed us to fine-tune the model to achieve better performance in sentiment classification tasks.\n",
    "\n",
    "5. **Challenges and Considerations**: During the project, we encountered challenges such as balancing model complexity with computational resources and managing overfitting. Regularization techniques like dropout and careful selection of hyperparameters were crucial in addressing these challenges.\n",
    "\n",
    "6. **Future Directions**: Moving forward, further enhancements could include exploring more advanced RNN variants (e.g., GRU, bi-directional LSTM), incorporating pre-trained word embeddings (e.g., Word2Vec, GloVe), or leveraging transfer learning techniques from larger language models (e.g., BERT, GPT). Additionally, expanding the dataset size and diversity could help generalize the model’s ability to handle different types of sentiment analysis tasks and domains.\n",
    "\n",
    "In conclusion, this project demonstrated the effectiveness of RNN-based architectures for sentiment analysis tasks and highlighted the importance of hyperparameter tuning in optimizing model performance. By continually refining and expanding upon these techniques, we can advance the state-of-the-art in natural language processing applications, particularly in sentiment analysis and related domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
